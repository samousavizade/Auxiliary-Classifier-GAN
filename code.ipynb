{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Required Directoies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "os.makedirs(\"CIFAR10\", exist_ok=True)\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"wandb\", exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set WANDB Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"1d6bdaf3f9f088abf0915e5e5cb6689e4c7e7476\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_classes, latent_dimension):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.latent_dimension = latent_dimension\n",
    "\n",
    "        self.label_embedding = nn.Embedding(n_classes, latent_dimension)\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=110, out_features=384, ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.tconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(384, 192, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.tconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(192, 96, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.tconv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(96, 48, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.tconv5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_noise: torch.Tensor, input_labels: torch.Tensor):\n",
    "        assert input_noise.dtype == torch.float32\n",
    "\n",
    "        generator_input = torch.mul(self.label_embedding(input_labels), input_noise)\n",
    "\n",
    "        x = generator_input\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = x.view((-1, 384, 1, 1,))\n",
    "\n",
    "        x = self.tconv2(x)\n",
    "        x = self.tconv3(x)\n",
    "        x = self.tconv4(x)\n",
    "        x = self.tconv5(x)\n",
    "\n",
    "        output_batch = x\n",
    "\n",
    "        return output_batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes, latent_dimension):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.latent_dimension = latent_dimension\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5, inplace=False),\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.conv3,\n",
    "            self.conv4,\n",
    "            self.conv5,\n",
    "            self.conv6,\n",
    "        )\n",
    "\n",
    "        self.fc7_discriminator = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.fc7_auxiliary = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 512, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_image):\n",
    "        x = batch_image\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.shape[0], 4 * 4 * 512)\n",
    "        predicted_validity_probability = self.fc7_discriminator(x)\n",
    "        predicted_auxiliary_probability = self.fc7_auxiliary(x)\n",
    "\n",
    "        return predicted_validity_probability, predicted_auxiliary_probability\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weights Initialization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def weights_init_normal(module, weights_init_type=\"xavier\"):\n",
    "    classname = module.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        if weights_init_type == \"xavier\":\n",
    "            torch.nn.init.xavier_normal_(module.weight.data, )\n",
    "\n",
    "        elif weights_init_type == \"he\":\n",
    "            torch.nn.init.kaiming_normal_(module.weight.data, )\n",
    "\n",
    "        else:\n",
    "            torch.nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        if weights_init_type == \"xavier\":\n",
    "            torch.nn.init.xavier_normal_(module.weight.data, )\n",
    "\n",
    "        elif weights_init_type == \"he\":\n",
    "            torch.nn.init.kaiming_normal_(module.weight.data, )\n",
    "\n",
    "        else:\n",
    "            torch.nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "\n",
    "        torch.nn.init.constant_(module.bias.data, 0.0)\n",
    "\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(module.bias.data, 0.0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def log_generated_images_in_grids(table, epoch, generator, n_row=2):\n",
    "    noises = torch.normal(0, 1, (n_row ** 2, hps[\"n_classes\"], hps[\"latent_dimension\"])).to(device)\n",
    "    labels = torch.arange(0, hps[\"n_classes\"], 1).repeat(n_row ** 2, 1).to(device)\n",
    "\n",
    "    generated_images = generator(noises, labels).reshape(n_row ** 2, hps[\"n_classes\"], 3, hps[\"image_size\"], hps[\"image_size\"])\n",
    "\n",
    "    grids = [make_grid(generated_images[:, n, :, :, :], padding=1, nrow=n_row, normalize=True) for n in range(hps[\"n_classes\"])]\n",
    "\n",
    "    table.add_data(\n",
    "        epoch,\n",
    "        *[wandb.Image(grid) for grid in grids]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check GPU Availability\n",
    "\n",
    "Check whether cuda is available and based on this, device object is built that is used in for pytorch tensors computation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper-Parameter Setting\n",
    "In this section, hyper-parameters that used in bert fine-tuning are defined. hyper-parameter optimization (HPO) will be done in the next parts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "hps = {\n",
    "    \"batch_size\": 500,\n",
    "    \"epochs\": 250,\n",
    "    \"image_size\": 32,\n",
    "    \"noise_standard_deviation\": 0.05,\n",
    "    \"workers\": 4,\n",
    "    \"n_classes\": 10,\n",
    "    \"latent_dimension\": 110,\n",
    "    \"weights_init_type\": \"normal\",\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment Track Initialization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.21"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/ahur4/Auxiliary-Classifier-GAN/wandb/run-20220811_184802-h51bxs5q</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/samousavizade/Auxiliary-Classifier-GAN-PyTorch/runs/h51bxs5q\" target=\"_blank\">experiment 1</a></strong> to <a href=\"https://wandb.ai/samousavizade/Auxiliary-Classifier-GAN-PyTorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "EXPERIMENT_NUM = 1\n",
    "run = wandb.init(\n",
    "    project=\"Auxiliary-Classifier-GAN-PyTorch\",\n",
    "    name=f\"experiment {EXPERIMENT_NUM}\",\n",
    "    config=hps)\n",
    "\n",
    "hps = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Initialization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(hps[\"image_size\"], interpolation=InterpolationMode.BICUBIC, ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=hps[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Initialization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "generator = Generator(n_classes=hps[\"n_classes\"], latent_dimension=hps[\"latent_dimension\"])\n",
    "discriminator = Discriminator(n_classes=hps[\"n_classes\"], latent_dimension=hps[\"latent_dimension\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weight Setting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "Discriminator(\n  (conv1): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (conv3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (conv4): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (conv5): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (conv6): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (conv_blocks): Sequential(\n    (0): Sequential(\n      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n      (2): Dropout(p=0.5, inplace=False)\n    )\n    (1): Sequential(\n      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      (3): Dropout(p=0.5, inplace=False)\n    )\n    (2): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      (3): Dropout(p=0.5, inplace=False)\n    )\n    (3): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      (3): Dropout(p=0.5, inplace=False)\n    )\n    (4): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      (3): Dropout(p=0.5, inplace=False)\n    )\n    (5): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      (3): Dropout(p=0.5, inplace=False)\n    )\n  )\n  (fc7_discriminator): Sequential(\n    (0): Linear(in_features=8192, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n  (fc7_auxiliary): Sequential(\n    (0): Linear(in_features=8192, out_features=10, bias=True)\n    (1): Softmax(dim=1)\n  )\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.apply(lambda module: weights_init_normal(module, weights_init_type=hps[\"weights_init_type\"]))\n",
    "discriminator.apply(lambda module: weights_init_normal(module, weights_init_type=hps[\"weights_init_type\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizer Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=hps[\"learning_rate\"], betas=(hps[\"beta1\"], hps[\"beta2\"]))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=hps[\"learning_rate\"], betas=(hps[\"beta1\"], hps[\"beta2\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Criteria Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CPU to GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 232/500 [1:55:27<2:52:55, 38.71s/it]\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      " 61%|██████    | 304/500 [2:32:39<1:35:26, 29.22s/it]\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      " 77%|███████▋  | 386/500 [3:14:26<57:21, 30.19s/it]  wandb: Network error (TransientError), entering retry loop.\n",
      " 78%|███████▊  | 388/500 [3:15:28<57:20, 30.71s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 78%|███████▊  | 390/500 [3:16:25<54:25, 29.69s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 80%|████████  | 400/500 [3:22:01<52:47, 31.68s/it]  wandb: Network error (TransientError), entering retry loop.\n",
      " 80%|████████  | 401/500 [3:22:32<51:34, 31.26s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 82%|████████▏ | 408/500 [3:26:04<46:26, 30.29s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 83%|████████▎ | 413/500 [3:28:32<42:39, 29.42s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 91%|█████████▏| 457/500 [3:50:51<22:04, 30.81s/it]wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      " 93%|█████████▎| 464/500 [3:54:28<18:10, 30.30s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 94%|█████████▍| 470/500 [3:57:43<16:15, 32.53s/it]wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      " 95%|█████████▌| 475/500 [4:00:17<13:07, 31.50s/it]wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      " 96%|█████████▌| 481/500 [4:03:13<09:28, 29.90s/it]wandb: Network error (TransientError), entering retry loop.\n",
      " 97%|█████████▋| 485/500 [4:05:12<07:31, 30.10s/it]wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "100%|██████████| 500/500 [4:13:00<00:00, 30.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<wandb.sdk.wandb_artifacts.Artifact at 0x7fcd7882d720>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = hps[\"batch_size\"]\n",
    "epochs = hps[\"epochs\"]\n",
    "batches = len(dataloader)\n",
    "\n",
    "wandb.watch([generator, discriminator], log=\"all\", log_freq=batches // 4)\n",
    "\n",
    "REAL = torch.zeros(batch_size, 1, requires_grad=False).fill_(1.0).to(device, )\n",
    "FAKE = torch.zeros(batch_size, 1, requires_grad=False).fill_(0.0).to(device, )\n",
    "\n",
    "columns = [\"Epoch\", *[f\"Label {i}\" for i in range(hps[\"n_classes\"])]]\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    generator_losses = torch.zeros(batches).to(device=device, dtype=torch.float)\n",
    "    discriminator_losses = torch.zeros(batches).to(device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    for batch, (batch_real_images, batch_real_labels) in enumerate(dataloader):\n",
    "        batch_real_images, batch_real_labels = batch_real_images.to(device, dtype=torch.float), batch_real_labels.to(device, dtype=torch.long)\n",
    "\n",
    "        # Generator Train\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Sample batch_noise and labels as generator input\n",
    "        # and Generate a batch of images\n",
    "        batch_noise = torch.normal(0, 1, (batch_size, hps[\"latent_dimension\"])).to(device)\n",
    "        batch_fake_generated_labels = torch.randint(0, hps[\"n_classes\"], (batch_size,)).to(device, dtype=torch.long)\n",
    "        batch_fake_generated_images = generator(batch_noise, batch_fake_generated_labels)\n",
    "\n",
    "        batch_predicted_validity_for_fake_images, batch_predicted_label_for_fake_images = discriminator(batch_fake_generated_images)\n",
    "\n",
    "        source_loss = adversarial_loss(batch_predicted_validity_for_fake_images, REAL)\n",
    "        class_loss = auxiliary_loss(batch_predicted_label_for_fake_images, batch_real_labels)\n",
    "\n",
    "        generator_loss = source_loss + class_loss\n",
    "        generator_losses[batch] = generator_loss\n",
    "\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Discriminator Train\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        batch_predicted_validity_for_real_images, batch_predicted_label_for_real_images = discriminator(batch_real_images)\n",
    "\n",
    "        source_loss_real = adversarial_loss(batch_predicted_validity_for_real_images, REAL)\n",
    "        class_loss_real = auxiliary_loss(batch_predicted_label_for_real_images, batch_real_labels)\n",
    "        discriminator_loss_real = source_loss_real + class_loss_real\n",
    "\n",
    "        batch_predicted_validity_for_fake_images, batch_predicted_label_for_fake_images = discriminator(batch_fake_generated_images.detach())\n",
    "\n",
    "        source_loss_fake = adversarial_loss(batch_predicted_validity_for_fake_images, FAKE)\n",
    "        class_loss_fake = auxiliary_loss(batch_predicted_label_for_fake_images, batch_fake_generated_labels)\n",
    "        discriminator_loss_fake = source_loss_fake + class_loss_fake\n",
    "\n",
    "        discriminator_loss = 0.5 * (discriminator_loss_real + discriminator_loss_fake)\n",
    "        discriminator_losses[batch] = discriminator_loss\n",
    "\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "    epoch_discriminator_loss = discriminator_losses.sum().item()\n",
    "    epoch_generator_loss = generator_losses.sum().item()\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    # Log Losses\n",
    "    wandb.log({\n",
    "        f\"Loss/Generator\": epoch_generator_loss,\n",
    "        f\"Loss/Discriminator\": epoch_discriminator_loss,\n",
    "    }, step=epoch)\n",
    "\n",
    "    # Generate Images Grid and Log\n",
    "    generated_images_table = wandb.Table(columns=columns)\n",
    "    log_generated_images_in_grids(generated_images_table, epoch, generator)\n",
    "    wandb.log({\"Generated Images Per Epoch\": generated_images_table, }, step=epoch)\n",
    "\n",
    "    # Save Model\n",
    "    if epoch % 50 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': generator_optimizer.state_dict(),\n",
    "            'loss': epoch_generator_loss,\n",
    "        }, f'models/generator_epoch{epoch}.pth')\n",
    "\n",
    "        # artifact = wandb.Artifact('generator', type='model')\n",
    "        # artifact.add_file(f'models/generator_epoch{epoch}.pth')\n",
    "        # run.log_artifact(artifact)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': discriminator_optimizer.state_dict(),\n",
    "            'loss': epoch_discriminator_loss,\n",
    "        }, f'models/discriminator_epoch{epoch}.pth')\n",
    "\n",
    "        # artifact = wandb.Artifact('discriminator', type='model')\n",
    "        # artifact.add_file(f'models/discriminator_epoch{epoch}.pth')\n",
    "        # run.log_artifact(artifact)\n",
    "\n",
    "\n",
    "# Save Final Model\n",
    "torch.save({\n",
    "    'model_state_dict': generator.state_dict(),\n",
    "    'optimizer_state_dict': generator_optimizer.state_dict(),\n",
    "}, f'models/generator.pth')\n",
    "\n",
    "artifact = wandb.Artifact('generator', type='model')\n",
    "artifact.add_file(f'models/generator.pth')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': discriminator.state_dict(),\n",
    "    'optimizer_state_dict': discriminator_optimizer.state_dict(),\n",
    "}, f'models/discriminator.pth')\n",
    "\n",
    "artifact = wandb.Artifact('discriminator', type='model')\n",
    "artifact.add_file(f'models/discriminator.pth')\n",
    "run.log_artifact(artifact)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Final Result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "n_row = 20\n",
    "noises = torch.normal(0, 1, (n_row, hps[\"n_classes\"], hps[\"latent_dimension\"])).to(device)\n",
    "labels = torch.arange(0, hps[\"n_classes\"], 1).repeat(n_row, 1).to(device)\n",
    "\n",
    "generated_images = generator(noises, labels).reshape(n_row * hps[\"n_classes\"], 3, hps[\"image_size\"], hps[\"image_size\"])\n",
    "\n",
    "# grids = [make_grid(generated_images[:, n, :, :, :], padding=1, nrow=n_row, normalize=True) for n in range(hps[\"n_classes\"])]\n",
    "grids = make_grid(generated_images[:, :, :, :], padding=2, nrow=n_row, normalize=True)\n",
    "save_image(grids, \"images/final_result.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<wandb.jupyter.IFrame at 0x7fcc5c95ecb0>",
      "text/html": "<iframe src=\"https://wandb.ai/USERNAME/PROJECT/workspace?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb USERNAME/PROJECT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}